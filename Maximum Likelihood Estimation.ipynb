{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE) Notes\n",
    "\n",
    "This document is based on the QuantEcon MLE notes. It also mirrors my own mathematical notes on MLE found here.\n",
    "\n",
    "**Motivation:** MLE allows us to fit a nonlinear model of endogenous exogenous relationships. This allows for a greater range of probabilistic relationships. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Theory\n",
    "\n",
    "MLE works by assuming some parametric (though one can also define a non-parametric MLE an example is bootstrapping) relationship between our variables. We then attempt to estimate the parameters of our model such that we \"maximize t he likelihood\" of seeing our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Sargent, consider the poisson distribution, given by the pmf:\n",
    "$$f(y) = \\frac{\\mu^y}{y!}e^{-\\mu}, \\; y = 0,1,\\dots,\\infty$$\n",
    "Then, we can condition the distribution on the vector of explanatory variables $\\textbf{x}_i$. Then using the poisson regression model,\n",
    "$$f(y_i | \\textbf{x}_i) = \\frac{\\mu_i^{y_i}}{y_i!}e^{-\\mu_i} \\; \\text{ where } \\mu_i = e^{\\textbf{x}_i \\beta} = e^{\\beta_0 + \\beta_1 x_{i,1} + \\dots + \\beta_k x_{i,k}}$$\n",
    "\n",
    "Recall that a poisson regression takes the form of $\\log{E(Y | \\textbf{x})} = \\boldsymbol{\\beta}'\\textbf{x} = E(Y | \\textbf{x}) = e^{\\boldsymbol{\\beta}'\\textbf{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual our parameter vector takes the following form:\n",
    "\n",
    "\\begin{split}\\boldsymbol{\\beta} = \\begin{bmatrix}\n",
    "                            \\beta_0 \\\\\n",
    "                            \\vdots \\\\\n",
    "                            \\beta_k \\\\\n",
    "                      \\end{bmatrix}\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking $\\boldsymbol{\\hat\\beta}$ that best fits the data. We can think of this as a joint distribution. So we construct a likelihood function, $\\mathcal{L}(\\boldsymbol{\\beta})$. Then given data $y_i = \\{y_1, y_2\\}$ and $y_i \\sim f(y_i)$ where $y_1$ and $y_2$ are independent. Thus,\n",
    "$$f(y_1, \\dots, y_n) = \\prod_{i=1}^n{f(y_n)}$$\n",
    "\n",
    "If we consider that this is a just a joint conditional poisson distribtuion we can rewrite it as:\n",
    "$$f(y_1,\\dots,y_n | x_1, \\dots, x_n; \\boldsymbol{\\beta}) = \\prod^n_{i=1}{\\frac{\\mu_i^{y_i}}{y_i!}e^{-\\mu_i} }$$\n",
    "\n",
    "This formulation shows that our outcome $y$ is conditional on $x$ and $\\beta$. Then since we are examining pmf we can say that this formulation gives us the probility that we would see out come $y$ given $x$ and $\\beta$. Notice, that is exactly what we want to maximize but in a different \"order\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, since we want our likelihood model to tell us the probobility of seeing a particular parameter vector given the data and outcome, we can define it as:\n",
    "$$\\mathcal{L}(\\beta\\, |\\, \\textbf{y,x}) = \\prod^n_{i=1}{\\frac{\\mu_i^{y_i}}{y_i!}e^{-\\mu_i} } = f(y_1,\\dots,y_n \\;|\\; x_1, \\dots, x_n; \\boldsymbol{\\beta})$$\n",
    "\n",
    "Thus, all we have done is treat the $\\beta$ vector as our random variable given the observations $(y, x)$.\n",
    "\n",
    "Then, all we are trying to do with MLE is:\n",
    "$$\\max_\\beta\\mathcal{L}(\\boldsymbol{\\beta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is difficult to maximize since it is a product and a single term can \"dominate\" our calculation. Thus, we usually take the log, and thus we maximize the log-likelihood. Recall that $\\log\\prod^n_{i=1}{X_i} = \\log(x_1\\cdot\\ldots\\cdot x_n) = \\log(x_1) + \\ldots +\\log(x_n) = \\sum^n_{i=1}{\\log{x_i}}$. \n",
    "\n",
    "So,\n",
    "$$\\log\\mathcal{L}(\\beta) = \\log(f(y_1,x_1;\\beta)\\cdot\\ldots\\cdot \\log(f(y_n,x_n;\\beta)) = \\sum^n_{i=1}\\log f(x,y;\\beta) = \\sum^n_{i=1}\\log(\\frac{\\mu_i^{y_i}}{y_i!}e^{-\\mu_i})$$\n",
    "$$=\\sum^n_{i=1} y_i\\log \\mu_i - \\sum^n_{i=1} \\mu_i - \\sum^n_{i=1}\\log y!$$\n",
    "\n",
    "The above is just an application of the properties of logarithims. Notice, that we are able to simplify $\\log e^{-\\mu_i}$ to $-\\mu_i$ because this is $\\log$ with base natrual $e$ (This is common in to use $\\log$ instead of $\\ln$, as is often taught in elementary algebra, to denote this). \n",
    "\n",
    "And, thus we can rewrite our maximization problem as:\n",
    "$$\\max_\\beta(\\sum^n_{i=1} y_i\\log \\mu_i - \\sum^n_{i=1} \\mu_i - \\sum^n_{i=1}\\log y!)$$\n",
    "\n",
    "To solve this we must use numerical optimization methods such as newton-raphson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
